# [논문 초고] 2. 서론 (Introduction)

## 2.1. 연구 배경 및 동기

인공지능 기술의 비약적인 발전으로 인해 단순히 사용자의 명령을 수행하는 수준을 넘어, 스스로 판단하고 다른 주체와 협력하거나 경쟁하는 ‘자율형 AI 에이전트(Autonomous AI Agents)’의 시대가 도래하고 있다. 대규모 언어 모델(LLM)을 두뇌로 하는 이러한 에이전트들은 금융, 의료, 군사 분야 등 복잡하고 윤리적인 판단이 요구되는 다양한 사회적 시스템에 통합될 것으로 예상된다.

특히, 다수의 AI 에이전트가 공존하는 환경에서는 에이전트 간의 상호작용이 필수적으로 발생한다. 기존의 연구들이 단일 모델의 윤리적 성향이나 바이어스(Bias)를 교정하는 정렬(Alignment) 기술에 집중해왔다면, 이제는 'AI 집단'이 어떻게 합의에 도달하고 그 과정에서 발생하는 집단적 특성이 무엇인지에 대한 탐구가 시급하다. 인간 사회에서 관찰되는 집단 사고(Groupthink), 동조 현상(Conformity), 필터 버블(Filter Bubble)과 같은 현상이 AI 집단에서도 재현될 수 있기 때문이다.

## 2.2. 연구의 목적

본 연구의 핵심 질문은 다음과 같다.

1. **의견 붕괴 현상:** 다양한 배경(Persona)을 가진 LLM 에이전트 집단이 정보가 공유되는 환경에서 독립적인 의사결정 능력을 유지할 수 있는가? 아니면 필연적으로 하나의 의견으로 수렴(Collapse)하는가?
2. **정보 노출의 영향:** 타인의 논리적 근거(Rationale)를 아는 것과 단순한 선택 결과(Stance)를 아는 것 중 무엇이 AI의 입장 변화에 더 강력한 영향을 미치는가?
3. **다양성 유지:** AI 집단의 사고 다양성을 보존하기 위해 필요한 상호작용의 경계는 어디까지인가?

## 2.3. 논문의 구성

본 논문은 다음과 같이 구성된다. 제3장에서는 관련 연구와 배경 이론을 고찰하고, 제4장에서는 실험 대상이 된 에이전트 설계 및 5가지 정보 전달 조건을 설명하는 실험 방법론을 서술한다. 제5장에서는 실험을 통해 얻어진 정량적 데이터를 바탕으로 한 엔트로피 변화 및 수렴 속도 분석 결과를 제시한다. 제6장에서는 에이전트들의 실제 답변 내용을 분석하여 정성적 변화 양상을 살피며, 제7장에서 실험 결과의 시사점과 한계를 논의한 뒤 제8장에서 최종 결론을 맺는다.
