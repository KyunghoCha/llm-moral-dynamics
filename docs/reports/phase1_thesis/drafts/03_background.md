# [논문 초고] 3. 배경 연구 (Background & Related Work)

## 3.1. LLM의 윤리적 가치 정렬 (Alignment)

최근 대규모 언어 모델(LLM) 연구에서 가장 중요한 화두 중 하나는 모델의 답변이 인간의 윤리적 가치와 일치하도록 만드는 ‘정렬(Alignment)’ 기술이다. RLHF(Reinforcement Learning from Human Feedback)와 같은 기법을 통해 모델은 유해한 답변을 피하고 공정하며 도움되는 정보를 제공하도록 훈련된다. 그러나 이러한 정렬은 주로 ‘개별 모델’의 입력을 제어하는 것에 집중되어 있어, 다수의 에이전트가 존재할 때 발생하는 사회적 역학(Social Dynamics)에 대해서는 충분히 다루지 못하고 있다.

## 3.2. 집단 지성과 집단 사고 (Group Intelligence vs. Groupthink)

인간 사회에서 집단 상호작용은 개별 전문성을 결합하여 더 나은 결론에 이르게 하는 ‘집단 지성’의 원천이 되기도 하지만, 역으로 특정 의견에 무비판적으로 동조하는 ‘집단 사고’나 특정 정보만 공유되는 ‘에코 체임버’를 형성하기도 한다. 특히 소셜 미디어나 추천 알고리즘에 의해 필터 버블이 강화되는 상황에서 정보의 편향적 노출은 민주적 다양성을 심각하게 저해한다. 본 연구는 이러한 인간의 사회 심리학적 현상이 데이터로 학습된 LLM 에이전트 세계에서도 동일하게, 혹은 더 극단적으로 나타나는지 검토한다.

## 3.3. 소셜 정보와 AI 합의 메커니즘

AI 에이전트들이 타인의 의견을 참고하여 자신의 입장을 수정하는 과정은 크게 정보적 영향(Informational Influence)과 규범적 영향(Normative Influence)으로 나뉜다. 정보적 영향은 타인의 논거가 합리적이라고 판단될 때 발생하며, 규범적 영향은 집단에서 소외되지 않기 위해 다수의 입장을 따를 때 발생한다. 본 논문의 실험 설계는 이러한 두 가지 영향력을 분리하여 관찰할 수 있도록 정보를 통제한다.
